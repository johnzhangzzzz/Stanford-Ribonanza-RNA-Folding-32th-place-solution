{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","import os, gc\n","import numpy as np\n","from sklearn.model_selection import KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","import neptune\n","from neptune.utils import stringify_unsupported"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fix fastai bug to enable fp16 training with dictionaries\n","\n","import torch\n","from fastai.vision.all import *\n","def flatten(o):\n","    \"Concatenate all collections and items as a generator\"\n","    for item in o:\n","        if isinstance(o, dict): yield o[item]; continue\n","        elif isinstance(item, str): yield item; continue\n","        try: yield from flatten(item)\n","        except TypeError: yield item\n","\n","from torch.cuda.amp import GradScaler, autocast\n","@delegates(GradScaler)\n","class MixedPrecision(Callback):\n","    \"Mixed precision training using Pytorch's `autocast` and `GradScaler`\"\n","    order = 10\n","    def __init__(self, **kwargs): self.kwargs = kwargs\n","    def before_fit(self): \n","        self.autocast,self.learn.scaler,self.scales = autocast(),GradScaler(**self.kwargs),L()\n","    def before_batch(self): self.autocast.__enter__()\n","    def after_pred(self):\n","        if next(flatten(self.pred)).dtype==torch.float16: self.learn.pred = to_float(self.pred)\n","    def after_loss(self): self.autocast.__exit__(None, None, None)\n","    def before_backward(self): self.learn.loss_grad = self.scaler.scale(self.loss_grad)\n","    def before_step(self):\n","        \"Use `self` as a fake optimizer. `self.skipped` will be set to True `after_step` if gradients overflow. \"\n","        self.skipped=True\n","        self.scaler.step(self)\n","        if self.skipped: raise CancelStepException()\n","        self.scales.append(self.scaler.get_scale())\n","    def after_step(self): self.learn.scaler.update()\n","\n","    @property \n","    def param_groups(self): \n","        \"Pretend to be an optimizer for `GradScaler`\"\n","        return self.opt.param_groups\n","    def step(self, *args, **kwargs): \n","        \"Fake optimizer step to detect whether this batch was skipped from `GradScaler`\"\n","        self.skipped=False\n","    def after_fit(self): self.autocast,self.learn.scaler,self.scales = None,None,None\n","        \n","import fastai\n","fastai.callback.fp16.MixedPrecision = MixedPrecision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","import argparse\n","from copy import copy\n","import importlib"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["BASEDIR= './'#'../input/asl-fingerspelling-config'\n","for DIRNAME in 'configs data models postprocess metrics utils repos'.split():\n","    sys.path.append(f'{BASEDIR}/{DIRNAME}/')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parser = argparse.ArgumentParser(description=\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#importlib.import_module(parser_args.config)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parser.add_argument(\"-C\", \"--config\", help=\"config filename\", default=\"cfg_0\")\n","parser.add_argument(\"-G\", \"--gpu_id\", default=\"\", help=\"GPU ID\")\n","parser_args, other_args = parser.parse_known_args(sys.argv)\n","#cfg = copy(importlib.import_module(parser_args.config).cfg)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cfg = copy(importlib.import_module('cfg_fastai').cfg)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_parquet(cfg.train_df)\n","BPPs_RNA_Dataset = importlib.import_module(cfg.dataset).BPPs_RNA_Dataset\n","LenMatchBatchSampler = importlib.import_module(cfg.dataset).LenMatchBatchSampler\n","DeviceDataLoader = importlib.import_module(cfg.dataset).DeviceDataLoader\n","Squeezeformer_RNA = importlib.import_module(cfg.model).Squeezeformer_RNA\n","loss = importlib.import_module(cfg.loss).loss\n","MAE=importlib.import_module(cfg.metrics).MAE\n","OUT=cfg.OUT\n","SEED=cfg.SEED\n","nfolds=cfg.nfolds\n","set_seed=importlib.import_module(cfg.utils).set_seed"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#pip install neptune-fastai\n","#from neptune.integrations.fastai import NeptuneCallback\n","#neptune_callback = NeptuneCallback(run=neptune_run)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["set_seed(SEED)\n","os.makedirs(OUT, exist_ok=True)\n","\n","    \n","for fold in [0]: #[0,1,2,3]\n","    ds_train = BPPs_RNA_Dataset(df, mode='train', fold=fold, nfolds = nfolds)\n","    ds_train_len = BPPs_RNA_Dataset(df, mode='train', fold=fold, \n","                nfolds=nfolds, mask_only=True)\n","    sampler_train = torch.utils.data.RandomSampler(ds_train_len)\n","    len_sampler_train = LenMatchBatchSampler(sampler_train, batch_size=cfg.bs,\n","                drop_last=True)\n","    dl_train = DeviceDataLoader(torch.utils.data.DataLoader(ds_train, \n","                batch_sampler=len_sampler_train, num_workers=cfg.num_workers,\n","                persistent_workers=True), cfg.device)\n","    ds_val = BPPs_RNA_Dataset(df, mode='eval', fold=fold, nfolds=nfolds)\n","    ds_val_len = BPPs_RNA_Dataset(df, mode='eval', fold=fold, nfolds=nfolds, \n","               mask_only=True)\n","    sampler_val = torch.utils.data.SequentialSampler(ds_val_len)\n","    len_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=cfg.bs, \n","               drop_last=False)\n","    dl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, \n","               batch_sampler=len_sampler_val, num_workers=cfg.num_workers), cfg.device)\n","    gc.collect()\n","    data = DataLoaders(dl_train,dl_val)\n","    model = Squeezeformer_RNA(cfg)\n","\n","    #model.load_state_dict(torch.load(m,map_location=torch.device('cpu')))\n","    model = model.to(cfg.device)\n","    learn = Learner(data, model, loss_func=loss,cbs=[GradientClip(3.0)], #neptune_callback\n","                metrics=[MAE()]).to_fp16()\n","    learn.fit_one_cycle(cfg.epochs, lr_max=cfg.lr, wd=cfg.weight_decay, pct_start=cfg.pct_start,)#pct_start=0.02\n","\n","    if not os.path.exists(f\"{cfg.output_dir}/fold{fold}/\"): \n","        os.makedirs(f\"{cfg.output_dir}/fold{fold}/\")\n","    torch.save(learn.model.state_dict(),f\"{cfg.output_dir}/fold{fold}/checkpoint_last_SEED{cfg.SEED}.pth\")\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":2}
